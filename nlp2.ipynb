{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is a #ball."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are studying nlp and this is the first lecture. we have completed supervised and unsupervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"we are studying nlp and this is the first lecture.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_without_stopwords = [word for word in tokens if word not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['studying', 'nlp', 'first', 'lecture', '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_without_punctuation = [word for word in tokens_without_stopwords if word.isalnum()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['studying', 'nlp', 'first', 'lecture']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_without_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming and lemmitization\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "p = PorterStemmer()\n",
    "\n",
    "stemed_tokens = [p.stem(word) for word in tokens_without_punctuation]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['studi', 'nlp', 'first', 'lectur']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "lemmitized_tokens = [lem.lemmatize(word) for word in tokens_without_punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['studying', 'nlp', 'first', 'lecture']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmitized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.stem('jumping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jumping'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem.lemmatize('jumping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['studying', 'nlp', 'first', 'lecture']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmitized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_tokens.toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('beautiful', 'NN')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(['beautiful'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part of speech\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_pos_tag(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0]\n",
    "    dict_tags ={'V': wordnet.VERB, 'N':wordnet.NOUN, 'R':wordnet.ADV,'J':wordnet.ADJ}\n",
    "    return dict_tags.get(tag,wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_tokens = [get_pos_tag(word) for word in lemmitized_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['v', 'n', 'r', 'n']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lemmitized_tokens = [lem.lemmatize(word, get_pos_tag(word)) for word in tokens_without_punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['study', 'nlp', 'first', 'lecture']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_lemmitized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bow\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer()\n",
    "\n",
    "vec_tokens = vec.fit_transform(lemmitized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"we are learning nlp\", \"this is the second class\", \"we are done wtih supervised learning\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "def clean_text(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tokens = [word for word in tokens if word not in stopwords and word not in string.punctuation]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sentences = [clean_text(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learning nlp', 'second class', 'done wtih supervised learning']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer()\n",
    "sentence_vectors  = vec.fit_transform(cleaned_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf\n",
    "\n",
    "#tf (t, d) = freq of t in d doc/no. of terms in d\n",
    "\n",
    "# idf(t, D) = no. of docs N/ no. of docs containing t\n",
    "\n",
    "#tf-idf = tf*idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vec =  TfidfVectorizer()\n",
    "\n",
    "sentence_vectors_tfidf = tfidf_vec.fit_transform(cleaned_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.60534851, 0.79596054, 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.70710678, 0.        , 0.        , 0.        , 0.70710678,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.52863461, 0.40204024, 0.        , 0.        ,\n",
       "        0.52863461, 0.52863461]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_vectors_tfidf.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learning nlp', 'second class', 'done wtih supervised learning']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['class', 'done', 'learning', 'nlp', 'second', 'supervised', 'wtih'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vec.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we are learning nlp',\n",
       " 'this is the second class',\n",
       " 'we are done wtih supervised learning']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec 2 layer nn \n",
    "#cbow -- predict current word using context\n",
    "#skip gram --> predict context using current word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learning nlp', 'second class', 'done wtih supervised learning']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "def clean_text(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tokens = [word for word in tokens if word not in stopwords and word not in string.punctuation]\n",
    "    #tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokens = [word_tokenize(sentence.lower()) for sentence in cleaned_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['learning', 'nlp'],\n",
       " ['second', 'class'],\n",
       " ['done', 'wtih', 'supervised', 'learning']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "    \n",
    "model = Word2Vec(sentences= sentence_tokens, vector_size = 50, window= 5, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learning', 'supervised', 'wtih', 'done', 'class', 'second', 'nlp']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.wv.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.0724545e-03,  4.7286271e-04,  1.0206699e-02,  1.8018546e-02,\n",
       "       -1.8605899e-02, -1.4233618e-02,  1.2917745e-02,  1.7945977e-02,\n",
       "       -1.0030856e-02, -7.5267432e-03,  1.4761009e-02, -3.0669428e-03,\n",
       "       -9.0732267e-03,  1.3108104e-02, -9.7203208e-03, -3.6320353e-03,\n",
       "        5.7531595e-03,  1.9837476e-03, -1.6570430e-02, -1.8897636e-02,\n",
       "        1.4623532e-02,  1.0140524e-02,  1.3515387e-02,  1.5257311e-03,\n",
       "        1.2701781e-02, -6.8107317e-03, -1.8928028e-03,  1.1537147e-02,\n",
       "       -1.5043275e-02, -7.8722071e-03, -1.5023164e-02, -1.8600845e-03,\n",
       "        1.9076237e-02, -1.4638334e-02, -4.6675373e-03, -3.8754821e-03,\n",
       "        1.6154874e-02, -1.1861792e-02,  9.0324880e-05, -9.5074680e-03,\n",
       "       -1.9207101e-02,  1.0014586e-02, -1.7519170e-02, -8.7836506e-03,\n",
       "       -7.0199967e-05, -5.9236289e-04, -1.5322480e-02,  1.9229487e-02,\n",
       "        9.9641159e-03,  1.8466286e-02], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['learning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vector(sentence):\n",
    "    tokens = word_tokenize(str(sentence.lower()))\n",
    "    word_vectors = [model.wv[word] for word in tokens if word in model.wv.index_to_key]\n",
    "    return numpy.mean(word_vectors, axis =0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "sentence_vectors_word2vec = [get_sentence_vector(sentence) for sentence in cleaned_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.00877891,  0.00953579,  0.00490569,  0.007042  , -0.00469932,\n",
       "        -0.01121213,  0.00920199,  0.01591296,  0.00105   , -0.01127417,\n",
       "         0.01676285,  0.00313834, -0.00057049,  0.00031055,  0.00359982,\n",
       "        -0.00396618,  0.01170177, -0.00437013, -0.01641463, -0.00262426,\n",
       "         0.00898296,  0.00287175,  0.01627129,  0.01025672, -0.00342316,\n",
       "        -0.00090014,  0.00521029,  0.00964103, -0.00549885, -0.0035056 ,\n",
       "        -0.00683795, -0.00475068,  0.00239787, -0.00940804,  0.00159013,\n",
       "         0.00688094,  0.01733659, -0.01190683, -0.00935751,  0.00501064,\n",
       "        -0.00617377,  0.01017341, -0.00247724, -0.00719609,  0.0072876 ,\n",
       "         0.00253409, -0.00479024,  0.00723437,  0.00185381,  0.006863  ],\n",
       "       dtype=float32),\n",
       " array([ 0.00153159,  0.00043131, -0.01388643, -0.00918077, -0.00145321,\n",
       "         0.00141124, -0.00552072, -0.00168117, -0.0147778 ,  0.00248816,\n",
       "        -0.00034816, -0.00690315,  0.01768953,  0.00240738,  0.00024838,\n",
       "         0.00335974,  0.01279172,  0.0154161 , -0.01530624,  0.00068699,\n",
       "        -0.00305574,  0.00032272,  0.00047734, -0.00659644,  0.00478544,\n",
       "         0.00350991,  0.01803579,  0.00905517, -0.00938702,  0.003965  ,\n",
       "         0.00668897, -0.01588486, -0.01562966, -0.00270625, -0.00738844,\n",
       "        -0.00732495, -0.00512871, -0.00404639,  0.0097705 , -0.00076012,\n",
       "        -0.01010346,  0.00743207,  0.00828264,  0.00269382,  0.01193827,\n",
       "         0.00990491,  0.00731343, -0.00218727,  0.00384627, -0.00805112],\n",
       "       dtype=float32),\n",
       " array([-4.74813068e-03, -5.56109590e-04,  2.97531299e-03,  9.52247158e-03,\n",
       "         2.86180852e-03, -4.68244450e-03,  1.05463853e-02,  7.38490839e-03,\n",
       "        -6.05779421e-03,  3.58600891e-03, -1.57085317e-03, -3.18697095e-03,\n",
       "        -9.45762638e-03,  4.43473691e-04,  2.86994502e-04,  6.62489329e-03,\n",
       "         8.27915687e-03,  4.14488558e-03, -3.69199272e-03, -5.62531408e-04,\n",
       "         5.70306834e-03,  7.57707749e-05,  9.56507586e-03, -5.89684257e-03,\n",
       "         8.34926683e-03, -3.31450999e-03, -2.76162522e-03,  3.42820911e-03,\n",
       "        -3.37818637e-03, -4.82148305e-03, -3.88667453e-03,  6.80408557e-04,\n",
       "         9.61234421e-03, -6.19535614e-03, -1.48875476e-03,  3.16414225e-04,\n",
       "         1.05408905e-02, -7.25568039e-03,  4.58420208e-03,  3.28360312e-03,\n",
       "         5.06820157e-04, -1.68425916e-03, -6.12256955e-03, -6.20568823e-03,\n",
       "         1.88335066e-03,  7.66096916e-03, -2.64668697e-03, -6.81997277e-04,\n",
       "        -1.05780829e-03,  1.09607987e-02], dtype=float32)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_vectors_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "news_df = pd.read_csv('news_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>REAL</td>\n",
       "      <td>Payal has accused filmmaker Anurag Kashyap of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>A four-minute-long video of a woman criticisin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>Republic Poll, a fake Twitter account imitatin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>REAL</td>\n",
       "      <td>Delhi teen finds place on UN green list, turns...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>REAL</td>\n",
       "      <td>Delhi: A high-level meeting underway at reside...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0  REAL  Payal has accused filmmaker Anurag Kashyap of ...\n",
       "1  FAKE  A four-minute-long video of a woman criticisin...\n",
       "2  FAKE  Republic Poll, a fake Twitter account imitatin...\n",
       "3  REAL  Delhi teen finds place on UN green list, turns...\n",
       "4  REAL  Delhi: A high-level meeting underway at reside..."
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3729, 2)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "stopwords = stopwords.words('english')\n",
    "def preprocess(sentence):\n",
    "    tokens = word_tokenize(str(sentence).lower())\n",
    "    tokens = [word for word in tokens if word not in stopwords and word not in string.punctuation]\n",
    "    return tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df['clean_text'] = news_df['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>REAL</td>\n",
       "      <td>Payal has accused filmmaker Anurag Kashyap of ...</td>\n",
       "      <td>[payal, accused, filmmaker, anurag, kashyap, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>A four-minute-long video of a woman criticisin...</td>\n",
       "      <td>[four-minute-long, video, woman, criticising, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>Republic Poll, a fake Twitter account imitatin...</td>\n",
       "      <td>[republic, poll, fake, twitter, account, imita...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>REAL</td>\n",
       "      <td>Delhi teen finds place on UN green list, turns...</td>\n",
       "      <td>[delhi, teen, finds, place, un, green, list, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>REAL</td>\n",
       "      <td>Delhi: A high-level meeting underway at reside...</td>\n",
       "      <td>[delhi, high-level, meeting, underway, residen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  \\\n",
       "0  REAL  Payal has accused filmmaker Anurag Kashyap of ...   \n",
       "1  FAKE  A four-minute-long video of a woman criticisin...   \n",
       "2  FAKE  Republic Poll, a fake Twitter account imitatin...   \n",
       "3  REAL  Delhi teen finds place on UN green list, turns...   \n",
       "4  REAL  Delhi: A high-level meeting underway at reside...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  [payal, accused, filmmaker, anurag, kashyap, b...  \n",
       "1  [four-minute-long, video, woman, criticising, ...  \n",
       "2  [republic, poll, fake, twitter, account, imita...  \n",
       "3  [delhi, teen, finds, place, un, green, list, t...  \n",
       "4  [delhi, high-level, meeting, underway, residen...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences = news_df['clean_text'], vector_size = 50, window = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13475"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.15160087e-01,  3.21336538e-01,  3.87219518e-01, -1.28771275e-01,\n",
       "       -4.44069244e-02, -6.34654999e-01,  6.00438893e-01, -3.50640416e-02,\n",
       "       -2.34462887e-01,  6.94480017e-02,  1.20690726e-01,  4.00881059e-02,\n",
       "       -3.86360705e-01, -1.32265612e-01,  1.05808727e-01, -1.19828299e-01,\n",
       "        1.22581162e-01,  2.96184152e-01, -1.92792654e-01, -2.94277251e-01,\n",
       "        7.55381137e-02,  4.73819524e-02,  3.70985091e-01,  3.82441729e-01,\n",
       "        1.82984337e-01, -6.45564124e-02, -2.58423924e-01, -2.42452119e-02,\n",
       "       -7.53795132e-02, -3.38794470e-01,  2.08803162e-01, -2.44364962e-01,\n",
       "       -2.38341242e-01, -2.96491027e-01, -2.28362769e-01,  3.36625817e-04,\n",
       "       -2.63036847e-01,  1.71666265e-01, -2.02304889e-02, -1.99135855e-01,\n",
       "        1.83768094e-01,  2.24796221e-01,  2.51994759e-01, -1.03595659e-01,\n",
       "        2.81693101e-01,  2.25900114e-02, -4.59422469e-01, -1.62699908e-01,\n",
       "       -5.30712083e-02,  1.06472991e-01], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['poll']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['payal',\n",
       " 'accused',\n",
       " 'filmmaker',\n",
       " 'anurag',\n",
       " 'kashyap',\n",
       " 'behaving',\n",
       " 'inappropriately',\n",
       " 'video',\n",
       " 'went',\n",
       " 'viral',\n",
       " 'maintained',\n",
       " 'stance',\n",
       " 'speaking',\n",
       " 'etimes',\n",
       " 'said',\n",
       " '“',\n",
       " 'wanted',\n",
       " 'speak',\n",
       " 'long',\n",
       " 'time',\n",
       " 'today',\n",
       " 'finally',\n",
       " 'thought',\n",
       " 'must',\n",
       " 'get',\n",
       " 'head',\n",
       " 'tweeted',\n",
       " 'incident',\n",
       " 'sometime',\n",
       " 'ago',\n",
       " 'metoo',\n",
       " 'movement',\n",
       " 'happened',\n",
       " 'many',\n",
       " 'people',\n",
       " 'told',\n",
       " 'delete',\n",
       " 'tweet',\n",
       " 'else',\n",
       " 'would',\n",
       " 'stop',\n",
       " 'getting',\n",
       " 'work',\n",
       " 'manager',\n",
       " 'advised',\n",
       " 'remove',\n",
       " 'tweet',\n",
       " 'complied',\n",
       " 'post',\n",
       " 'anurag',\n",
       " 'blocked',\n",
       " 'whatsapp',\n",
       " '”']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.loc[0,'clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(tokens):\n",
    "    word_vector = [model.wv[word] for word in tokens if word in model.wv.index_to_key]\n",
    "    sentence_vector = numpy.mean(word_vector, axis =0)\n",
    "    return sentence_vector\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_news_df = news_df[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_18496\\1904817262.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sample_news_df['word2vec_vector']  = sample_news_df['clean_text'].apply(get_vector)\n"
     ]
    }
   ],
   "source": [
    "sample_news_df['word2vec_vector']  = sample_news_df['clean_text'].apply(get_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>word2vec_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Payal has accused filmmaker Anurag Kashyap of ...</td>\n",
       "      <td>[payal, accused, filmmaker, anurag, kashyap, b...</td>\n",
       "      <td>[0.43928933, 0.27097186, 0.13300538, -0.935865...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>A four-minute-long video of a woman criticisin...</td>\n",
       "      <td>[four-minute-long, video, woman, criticising, ...</td>\n",
       "      <td>[-0.19309464, 0.8914795, 0.6746488, 0.1528777,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>Republic Poll, a fake Twitter account imitatin...</td>\n",
       "      <td>[republic, poll, fake, twitter, account, imita...</td>\n",
       "      <td>[0.2979288, 0.7908584, 0.8854035, -0.15631333,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Delhi teen finds place on UN green list, turns...</td>\n",
       "      <td>[delhi, teen, finds, place, un, green, list, t...</td>\n",
       "      <td>[-0.04496358, 0.19419488, -0.18477531, -0.2292...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Delhi: A high-level meeting underway at reside...</td>\n",
       "      <td>[delhi, high-level, meeting, underway, residen...</td>\n",
       "      <td>[-0.51848423, 0.14019454, -0.3519918, -0.82751...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text  \\\n",
       "0      1  Payal has accused filmmaker Anurag Kashyap of ...   \n",
       "1     -1  A four-minute-long video of a woman criticisin...   \n",
       "2     -1  Republic Poll, a fake Twitter account imitatin...   \n",
       "3      1  Delhi teen finds place on UN green list, turns...   \n",
       "4      1  Delhi: A high-level meeting underway at reside...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  [payal, accused, filmmaker, anurag, kashyap, b...   \n",
       "1  [four-minute-long, video, woman, criticising, ...   \n",
       "2  [republic, poll, fake, twitter, account, imita...   \n",
       "3  [delhi, teen, finds, place, un, green, list, t...   \n",
       "4  [delhi, high-level, meeting, underway, residen...   \n",
       "\n",
       "                                     word2vec_vector  \n",
       "0  [0.43928933, 0.27097186, 0.13300538, -0.935865...  \n",
       "1  [-0.19309464, 0.8914795, 0.6746488, 0.1528777,...  \n",
       "2  [0.2979288, 0.7908584, 0.8854035, -0.15631333,...  \n",
       "3  [-0.04496358, 0.19419488, -0.18477531, -0.2292...  \n",
       "4  [-0.51848423, 0.14019454, -0.3519918, -0.82751...  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_18496\\3453965754.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sample_news_df['label'] = sample_news_df['label'].map(label_encoding)\n"
     ]
    }
   ],
   "source": [
    "label_encoding = {'REAL':1, 'FAKE':-1}\n",
    "\n",
    "sample_news_df['label'] = sample_news_df['label'].map(label_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sample_news_df['word2vec_vector']\n",
    "y = sample_news_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60,)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "clf.fit(x_train.to_list(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(x_test.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the entered news is fake\n"
     ]
    }
   ],
   "source": [
    "user_text = input(\"enter some news to verify its authenticity: \")\n",
    "clean_user_text = preprocess(user_text)\n",
    "user_vector = get_vector(clean_user_text)\n",
    "final_prediction = clf.predict(user_vector.reshape(1,-1))\n",
    "if final_prediction[0]==-1:\n",
    "    print(\"the entered news is fake\")\n",
    "else:\n",
    "    print(\"the news is real\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.5767726 ,  0.4585576 ,  0.75611454,  0.05270781, -0.3146547 ,\n",
       "       -1.0885375 ,  1.9330652 ,  0.81175894, -1.7773981 , -1.0639175 ,\n",
       "        0.314969  , -0.2696809 , -0.90806824, -0.06326166,  0.28001642,\n",
       "       -1.2410063 ,  1.0264688 ,  1.1544445 , -0.19873065, -1.8986737 ,\n",
       "       -0.307828  , -0.5869638 ,  1.6938643 ,  1.3751187 , -2.2732751 ,\n",
       "        0.91253835, -1.1411344 ,  1.0715941 ,  1.031589  , -0.72680986,\n",
       "       -0.5332267 , -0.70994204, -0.9737197 , -0.7682363 , -0.9130855 ,\n",
       "       -0.33617756, -2.0397074 , -0.7591959 ,  0.36878446, -0.6120846 ,\n",
       "        1.7741998 ,  0.12396903,  0.29287052,  0.34505898, -0.17587379,\n",
       "        0.9338138 , -0.9742284 , -1.1012169 , -2.0643594 ,  0.25534034],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1], dtype=int64)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('screen', 0.8789852857589722),\n",
       " ('grab', 0.8351165056228638),\n",
       " ('clipping', 0.782625138759613),\n",
       " ('example', 0.7778211236000061),\n",
       " ('unverified', 0.7724040746688843),\n",
       " ('actually', 0.7717491984367371),\n",
       " ('screenshots', 0.7686144113540649),\n",
       " ('graphic', 0.7637912034988403),\n",
       " ('display', 0.7634642720222473),\n",
       " ('shown', 0.7599057555198669)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('real')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classfier = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
